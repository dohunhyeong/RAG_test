import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_teddynote.korean import stopwords
from langchain_teddynote.community.pinecone import init_pinecone_index
from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever
from langchain_core.prompts import PromptTemplate
import pickle
import base64


st.title("ë²•ì •ê°ì—¼ë³‘ Q&A ğŸ’¬")

openai_api_key = st.secrets["api_keys"]["openai"]
pinecone_api_key = st.secrets["api_keys"]["pinecone"]


# sparse_encoder ë””ì½”ë”© ë¶€ë¶„ì„ try-exceptë¡œ ê°ì‹¸ì„œ ì˜¤ë¥˜ í™•ì¸
try:
    sparse_encoder = pickle.loads(base64.b64decode(st.secrets["pickle_data"]["sparse_encoder"]))
    st.write("Sparse encoder loaded successfully")  # ë””ë²„ê¹…ìš©
except Exception as e:
    st.error(f"Error loading sparse encoder: {str(e)}")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°
    st.session_state["chain"] = None



# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


def create_retriever():
    pinecone_params = init_pinecone_index(
    index_name="dohun1",  # Pinecone ì¸ë±ìŠ¤ ì´ë¦„
    namespace="dohun_first_trial",  # Pinecone Namespace
    api_key=pinecone_api_key,  # Pinecone API Key
    sparse_encoder_path=sparse_encoder,  # Sparse Encoder ì €ì¥ê²½ë¡œ(save_path)
    stopwords=stopwords(),  # ë¶ˆìš©ì–´ ì‚¬ì „
    tokenizer="kiwi",
    embeddings= OpenAIEmbeddings(openai_api_key=openai_api_key), # Dense Embedder
    top_k=3,  # Top-K ë¬¸ì„œ ë°˜í™˜ ê°œìˆ˜
    alpha=0.5,  # alpha=0.75ë¡œ ì„¤ì •í•œ ê²½ìš°, (0.75: Dense Embedding, 0.25: Sparse Embedding)
    )
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒë¼ë¯¸í„° ì¶œë ¥ (ë¯¼ê°ì •ë³´ëŠ” ì œì™¸)
    debug_params = {k: v for k, v in pinecone_params.items() 
                   if k not in ['api_key', 'sparse_encoder_path']}
    st.write("Pinecone parameters:", debug_params)
    
    try:
        pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)
        st.write("Retriever created successfully")  # ë””ë²„ê¹…ìš©
        return pinecone_retriever
    except Exception as e:
        st.error(f"Error creating retriever: {str(e)}")
        raise


# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="gpt-4o"):
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = PromptTemplate(
        input_variables=["question", "context"],
        template="""
        You are an assistant for question-answering tasks. 
        Use the following pieces of retrieved context to answer the question. 
        If you don't know the answer, just say that you don't know. 
        Please write your answer in a markdown table format with the main points.
        Be sure to include your source in your answer.
        Answer in Korean.

        # Example Format:
        (brief summary of the answer)
        (table)
        (detailed answer to the question)
        
        **ì¶œì²˜**
        - (URL of the source)

        # Question: 
        {question}

        # Context: 
        {context}

        # Answer:
        """
    )
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model=model_name, 
                     temperature=0,
                     openai_api_key = openai_api_key)

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

custom_retriever = create_retriever()

chain = create_chain(custom_retriever, model_name='gpt-4o')
st.session_state["chain"] = chain

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # chain ì„ ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        # íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        warning_msg.error("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
